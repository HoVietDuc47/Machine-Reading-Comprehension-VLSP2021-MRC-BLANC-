{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[train]_1_algin_segmented_answer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ETvSByTAoYWI"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YY-6kLM6oZBF"},"source":["%cd '/content/gdrive/MyDrive/VLSP2021-MRC-BLANC'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bAmuwPxTgK9s"},"source":["!pip install transformers trankit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGzPDzVkiJaA"},"source":["import string\n","import json\n","from trankit import Pipeline\n","from tqdm import tqdm\n","\n","punctuation = string.punctuation + 'â€¦'\n","\n","p = Pipeline('vietnamese', embedding='xlm-roberta-large')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dtdjPqwZnbI0"},"source":["def algin_segmented_answer(tokenized_context, answer):\n","    answer_start = answer['answer_start']\n","    answer_text = ' '.join(answer['text'].split())\n","    answer_end = answer_start + len(answer_text)\n","\n","    left_spans, right_spans = [], []\n","    for sentence in tokenized_context['sentences']:\n","        for token in sentence['tokens']:\n","            left_spans.append(token['dspan'][0])\n","            right_spans.append(token['dspan'][1])\n","    \n","    if answer_start is not None and answer_start not in left_spans:\n","        corrected_answer_start = left_spans[0]\n","        corrected_answer_start_idx = 0\n","        for idx, left_span in enumerate(left_spans):\n","            if left_span > answer_start: break\n","            corrected_answer_start = left_span\n","            corrected_answer_start_idx = idx\n","        \n","        if corrected_answer_start_idx + 1 < len(left_spans) and ((corrected_answer_start + left_spans[corrected_answer_start_idx + 1])/2 < answer_start):\n","            answer_start = left_spans[corrected_answer_start_idx + 1]\n","        else:\n","            answer_start = corrected_answer_start\n","\n","    if answer_start is not None and answer_end not in right_spans:\n","        corrected_answer_end = right_spans[-1]\n","        corrected_answer_end_idx = 0\n","        for idx, right_span in enumerate(right_spans[::-1]):\n","            if right_span < answer_end: break\n","            corrected_answer_end = right_span\n","            corrected_answer_end_idx = idx\n","\n","        if corrected_answer_end_idx + 1 < len(right_spans) and ((corrected_answer_end + right_spans[::-1][corrected_answer_end_idx + 1])/2 < answer_end):\n","            answer_end = right_spans[::-1][corrected_answer_end_idx + 1]\n","        else:\n","            answer_end = corrected_answer_end\n","\n","    if answer_start is not None:\n","        while answer_start < len(tokenized_context['text']):\n","            if (tokenized_context['text'][answer_start] in punctuation or tokenized_context['text'][answer_start] == ' '): answer_start += 1\n","            else: break\n","        while answer_start not in left_spans: answer_start -= 1\n","\n","        while answer_end - 1 > 0:\n","            if tokenized_context['text'][answer_end - 1] in punctuation or tokenized_context['text'][answer_end - 1] == ' ': answer_end -= 1\n","            else: break\n","        while answer_end not in right_spans: answer_end += 1\n","    \n","    cur_offset, new_answer_start, new_answer_end, new_answer_text = 0, None, None, []\n","    for sentence in tokenized_context['sentences']:\n","        for token in sentence['tokens']:\n","            if token['dspan'][0] == answer_start: new_answer_start = cur_offset\n","            if new_answer_start is not None and new_answer_end is None: new_answer_text.append(token['text'].replace(' ', '_'))\n","            if token['dspan'][1] == answer_end: new_answer_end = cur_offset\n","            cur_offset = cur_offset + token['dspan'][1] - token['dspan'][0] + 1\n","    \n","    new_answer_text = ' '.join(new_answer_text)\n","\n","    return {'answer_start': new_answer_start, 'text': new_answer_text}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jHPjl-J_okMl"},"source":["original_data = json.load(open('./data/train_data/original_data/train.json', 'r'))\n","\n","for article in tqdm(original_data['data']):\n","    title = article['title']\n","    for paragraph in article['paragraphs']:\n","        context = paragraph['context']\n","        context = ' '.join(context.split())\n","        tokenized_context = p.tokenize(context)\n","        for qa in paragraph['qas']:\n","            question = qa['question']\n","            question = ' '.join(question.split())\n","            tokenized_question = p.tokenize(question, is_sent=True)\n","            qa['question'] = ' '.join([token['text'].replace(' ', '_') for token in tokenized_question['tokens']])\n","            qa['answers'] = [algin_segmented_answer(tokenized_context, answer) for answer in qa['answers']]\n","            if 'plausible_answers' in qa:\n","                qa['plausible_answers'] = [algin_segmented_answer(tokenized_context, answer) for answer in qa['plausible_answers']]\n","\n","        paragraph['context'] = ' '.join([token['text'].replace(' ', '_') for sentence in tokenized_context['sentences'] for token in sentence['tokens']])\n","\n","json.dump(original_data, open(f'./data/train_data/tokenized_data/segmented_data.json', 'w', encoding='utf-8'), ensure_ascii=False)"],"execution_count":null,"outputs":[]}]}