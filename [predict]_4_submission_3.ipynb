{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[predict]_4_submission3.ipynb","provenance":[],"collapsed_sections":["W9x8p9D-uOH6","adtzW-XGx4KI","tQa8SARWyoRf","qL97JEA3496X","Zil2dqCC2njE"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"o_uDxHyhoTAf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635413284929,"user_tz":-420,"elapsed":17142,"user":{"displayName":"Hang Le Thi Thu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01131064715747594997"}},"outputId":"3d4db418-a174-47ea-e706-095fc4a26452"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"mlfKYyrBodKN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635413294309,"user_tz":-420,"elapsed":9384,"user":{"displayName":"Hang Le Thi Thu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01131064715747594997"}},"outputId":"c140e859-8cd8-42d9-b5f4-aaac8acc9c72"},"source":["!pip install datasets sacremoses huggingface_hub tokenizers sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n","\u001b[K     |████████████████████████████████| 290 kB 5.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 41.9 MB/s \n","\u001b[?25hCollecting huggingface_hub\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 4.2 MB/s \n","\u001b[?25hCollecting tokenizers\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 37.1 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 56.3 MB/s \n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 48.4 MB/s \n","\u001b[?25hCollecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 72.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 56.1 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.3.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.13)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 70.6 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Collecting async-timeout<4.0,>=3.0\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 71.3 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, huggingface-hub, tokenizers, sentencepiece, sacremoses, datasets\n","Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.14.0 fsspec-2021.10.1 huggingface-hub-0.0.19 multidict-5.2.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.10.3 xxhash-2.0.2 yarl-1.7.0\n"]}]},{"cell_type":"code","metadata":{"id":"Uay9-3wNofQm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635413294311,"user_tz":-420,"elapsed":18,"user":{"displayName":"Hang Le Thi Thu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01131064715747594997"}},"outputId":"489d3c36-415b-4e6c-e121-d6de60f1b7d7"},"source":["cd /content/drive/MyDrive/VLSP2021-MRC-BLANC"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1WQcuxpxeN4XEvs7OGyXFqtypm6n8iNvd/VLSP2021-MRC-BLANC\n"]}]},{"cell_type":"markdown","metadata":{"id":"W9x8p9D-uOH6"},"source":["## Predict 10-fold"]},{"cell_type":"code","metadata":{"id":"AJ5PXoUiuP4S"},"source":["!python run_squad.py \\\n","  --model_type xlm-roberta \\\n","  --model_name_or_path ./models/finetuned10_models/xlm-roberta-large-fold_1 \\\n","  --do_eval \\\n","  --predict_file ./data/private_test_data/private_test_syllable.json \\\n","  --per_gpu_train_batch_size 4 \\\n","  --per_gpu_eval_batch_size 4 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3.0 \\\n","  --max_seq_length 384 \\\n","  --max_answer_length 500 \\\n","  --max_query_length 128 \\\n","  --doc_stride 128 \\\n","  --geometric_p 0.7 \\\n","  --window_size 2 \\\n","  --lmb 0.4 \\\n","  --logging_steps 500 \\\n","  --save_steps 500 \\\n","  --version_2_with_negative \\\n","  --cache_dir  ./results/private_test_result/10_fold_new/xlm-roberta-large-fold_1\n","  --output_dir ./results/private_test_result/10_fold_new/xlm-roberta-large-fold_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J2GOPaWkuUNw"},"source":["!python run_squad.py \\\n","  --model_type rembert \\\n","  --model_name_or_path ./models/finetuned10_models/rembert-fold_1 \\\n","  --do_eval \\\n","  --predict_file ./data/private_test_data/private_test_syllable.json \\\n","  --per_gpu_eval_batch_size 2 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3.0 \\\n","  --max_answer_length 500 \\\n","  --max_seq_length 384 \\\n","  --max_query_length 128 \\\n","  --doc_stride 128 \\\n","  --geometric_p 0.7 \\\n","  --window_size 2 \\\n","  --lmb 0.4 \\\n","  --logging_steps 500 \\\n","  --save_steps 0 \\\n","  --version_2_with_negative \\\n","  --overwrite_cache \\\n","  --cache_dir ./results/private_test_result/10_fold_new/rembert-fold_1 \\\n","  --output_dir ./results/private_test_result/10_fold_new/rembert-fold_1\\"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"adtzW-XGx4KI"},"source":["## Ensemble 10-fold"]},{"cell_type":"code","metadata":{"id":"1FzYSl4r5Y_X"},"source":["input_nbest_files_rembert = [f\"./results/private_test_result/10_fold/rembert-fold_{i+1}/nbest_predictions_.json\" for i in range(10)]\n","input_nbest_files_xlm = [f\"./results/private_test_result/10_fold/xlm-roberta-large-fold_{i+1}/nbest_predictions_.json\" for i in range(10)]\n","input_nbest_files=input_nbest_files_rembert + input_nbest_files_xlm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UVB4voD3GJH7"},"source":["import collections\n","import json\n","\n","idx = 0\n","best_cof = [0.5/10 for _ in range(10)] + [0.5/10 for _ in range(10)]\n"," \n","all_nbest = collections.OrderedDict()\n","for input_file in input_nbest_files:\n","    with open(input_file, \"r\") as reader:\n","        input_data = json.load(reader, strict=False)\n","        for (key, entries) in input_data.items():\n","            if key not in all_nbest:\n","                all_nbest[key] = collections.defaultdict(float)\n","            for entry in entries:\n","                all_nbest[key][(entry[\"text\"], entry[\"new_context\"])] += best_cof[idx] * entry[\"probability\"]\n","    idx += 1\n","\n","output_predictions = {}\n","for (key, entry_map) in all_nbest.items():\n","    sorted_texts = sorted(\n","        entry_map.keys(), key=lambda x: entry_map[x], reverse=True)\n","    best_text = sorted_texts[0]\n","    output_predictions[key] = best_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iY-uH3fB0ilW"},"source":["with open(\"/content/results_pred.json\", \"w\", encoding='utf-8') as write_file:\n","  json.dump(output_predictions, write_file, indent=4, ensure_ascii=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zyMmYS__Svuy"},"source":["raw_data = json.load(open('./data/private_test_data/private_test_syllable.json'))\n","\n","new_data_train = {'version': \"Top 1\", 'data': []}\n","for id_a, dt in enumerate(raw_data['data']):\n","  new_data_train['data'].append({\"title\": dt['title'], \"paragraphs\": []})\n","  for para in dt['paragraphs']:\n","    for qa in para['qas']:\n","      question = qa['question']\n","      id = qa['id']\n","\n","      new_context = output_predictions[id][1]\n","      answer = output_predictions[id][0]\n","      is_impossible = True\n","      if answer == '':\n","        continue\n","\n","      answers = [{'answer_start': 0, 'text': answer}]\n","      qas_ = [{'question': question, 'answers': [], 'id': qa['id'], 'is_impossible': is_impossible}]\n","      new_data_train['data'][id_a][\"paragraphs\"].append({'context':new_context, 'qas':qas_   })\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gmE8mjVkH3Y3"},"source":["with open(\"./data/top1_data/private_data_top1.json\", \"w\", encoding='utf-8') as write_file:\n","  json.dump(new_data_train, write_file, indent=4, ensure_ascii=False) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tQa8SARWyoRf"},"source":["## Predict 10-fold top1"]},{"cell_type":"code","metadata":{"id":"PCBufIacuapt"},"source":["!python run_squad.py \\\n","  --model_type xlm-roberta \\\n","  --model_name_or_path ./models/finetuned10_top1_models/xlm-roberta-large-fold_1 \\\n","  --do_eval \\\n","  --predict_file ./data/private_test_data/private_test_syllable.json \\\n","  --per_gpu_train_batch_size 4 \\\n","  --per_gpu_eval_batch_size 4 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3.0 \\\n","  --max_seq_length 384 \\\n","  --max_answer_length 500 \\\n","  --max_query_length 128 \\\n","  --doc_stride 128 \\\n","  --geometric_p 0.7 \\\n","  --window_size 2 \\\n","  --lmb 0.4 \\\n","  --logging_steps 500 \\\n","  --save_steps 500 \\\n","  --version_2_with_negative \\\n","  --cache_dir  ./results/private_test_result/10_fold_top1/xlm-roberta-large-fold_1\n","  --output_dir ./results/private_test_result/10_fold_top1/xlm-roberta-large-fold_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q1Ny1YrazKsF"},"source":["!python run_squad.py \\\n","  --model_type rembert \\\n","  --model_name_or_path ./models/finetuned10_top1_models/rembert-fold_1 \\\n","  --do_eval \\\n","  --predict_file ./data/private_test_data/private_test_syllable.json \\\n","  --per_gpu_eval_batch_size 2 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3.0 \\\n","  --max_answer_length 500 \\\n","  --max_seq_length 384 \\\n","  --max_query_length 128 \\\n","  --doc_stride 128 \\\n","  --geometric_p 0.7 \\\n","  --window_size 2 \\\n","  --lmb 0.4 \\\n","  --logging_steps 500 \\\n","  --save_steps 0 \\\n","  --version_2_with_negative \\\n","  --overwrite_cache \\\n","  --cache_dir ./results/private_test_result/10_fold_top1/rembert-fold_1 \\\n","  --output_dir ./results/private_test_result/10_fold_top1/rembert-fold_1\\"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qL97JEA3496X"},"source":["## Ensemble 10-fold top1"]},{"cell_type":"code","metadata":{"id":"_45Kzmmg0GaW"},"source":["input_nbest_files_xlm = [f\"./results/private_test_result/10_fold_top1/xlm-roberta-large-fold_{i+1}/nbest_predictions_.json\" for i in range(10)]\n","input_nbest_files_rembert = [f\"./results/private_test_result/10_fold_top1/rembert-fold_{i+1}/nbest_predictions_.json\" for i in range(10)]\n","input_nbest_files=input_nbest_files_rembert + input_nbest_files_xlm #Tổng hợp các file nbest_predictions_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pxw_n2P747iO"},"source":["import collections\n","import json\n","\n","idx = 0\n","best_cof = [0.5/10 for _ in range(10)] + [0.5/10 for _ in range(10)]\n"," \n","all_nbest = collections.OrderedDict()\n","for input_file in input_nbest_files:\n","    with open(input_file, \"r\") as reader:\n","        input_data = json.load(reader, strict=False)\n","        for (key, entries) in input_data.items():\n","            if key not in all_nbest:\n","                all_nbest[key] = collections.defaultdict(float)\n","            for entry in entries:\n","                all_nbest[key][entry[\"text\"]] += best_cof[idx] * entry[\"probability\"]\n","    idx += 1\n","\n","output_predictions = {}\n","for (key, entry_map) in all_nbest.items():\n","    sorted_texts = sorted(\n","        entry_map.keys(), key=lambda x: entry_map[x], reverse=True)\n","    best_text = sorted_texts[0]\n","    output_predictions[key] = best_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"__m1qxJGnvZ_"},"source":["file_out = \"/content/results_pred_2.json\"\n","with open(file_out, 'w', encoding='utf-8') as outfile:\n","    json.dump(output_predictions, outfile ,ensure_ascii=False,indent=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zil2dqCC2njE"},"source":["## Assign null answers among duplicate answers using top1 models"]},{"cell_type":"code","metadata":{"id":"VWHwiA7cguda"},"source":["pred_10fold = json.load(open(\"/content/results_pred.json\"))\n","pred_10fold_top1 = json.load(open(\"/content/results_pred_2.json\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MhPDk4p-2YI-"},"source":["def group_ans_in_context(test_file):\n","  public_test = json.load(open(test_file))['data']\n","  i = 0\n","  data_id = {}\n","  for dt in public_test:\n","    for para in dt['paragraphs']:\n","      a = []\n","      for qa in para['qas']:\n","        a.append(qa['id'])\n","      data_id[i] = a\n","      i += 1\n","  return data_id\n"," \n","def find_same_predict_answer(data_id, data ):\n","    occurrences = lambda s, lst: (i for i,e in enumerate(lst) if e == s)\n","    dct_final = {}\n","    for key, values in data_id.items():\n","        predict = []\n","        for value in values:\n","          predict.append(data[value])\n","        lst_same_ans = []\n","        flag = False\n","        for value in values:\n","          if data[value][0] != \"\":\n","            lst = list(occurrences(data[value], predict))\n","            if len(lst) > 1:\n","              d = []\n","              for i in lst:\n","                flag = True\n","                d.append(values[i])\n","              if d not in lst_same_ans:\n","                lst_same_ans.append(d)\n","        if flag == True:\n","          dct_final[key] = lst_same_ans\n","        flag = False\n","    return dct_final\n","\n","def find_answer_same_predict_ans(prediction, test_file_path):\n","    data_id = group_ans_in_context(test_file_path) # Group answers id in the same context together\n","    dct_final = find_same_predict_answer(data_id, prediction) # Find the probability in the same answer in each context\n","    return dct_final\n","\n","same_ans = find_answer_same_predict_ans(pred_10fold,\"./data/private_test_data/private_test_syllable.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S92mIAAm58Pw"},"source":["list_null_top1 = [k for k, v in pred_10fold_top1.items() if v == \"\"]\n","null_list_same_ans = []\n","for _, values in same_ans.items():\n","  for value in values:\n","    for v in value:\n","      if v in list_null_top1:\n","        null_list_same_ans.append(v)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JcmlSBSgwcy"},"source":["i = 0\n","new_data = {}\n","for id, ans in pred_10fold.items():\n","  if id in null_list_same_ans:\n","    new_data[id] = \"\"\n","  else:\n","    new_data[id] = pred_10fold[id][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dvdF4mlK10YG"},"source":["file_out = \"/content/results.json\"\n","with open(file_out, 'w', encoding='utf-8') as outfile:\n","    json.dump(new_data, outfile ,ensure_ascii=False,indent=4)"],"execution_count":null,"outputs":[]}]}