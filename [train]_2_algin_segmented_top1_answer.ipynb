{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[train]_2_algin_segmented_top1_answer.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ETvSByTAoYWI"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YY-6kLM6oZBF"},"source":["%cd '/content/gdrive/MyDrive/VLSP2021-MRC-BLANC'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bAmuwPxTgK9s"},"source":["!pip install transformers trankit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGzPDzVkiJaA"},"source":["import string\n","import json\n","from trankit import Pipeline\n","from tqdm import tqdm\n","\n","punctuation = string.punctuation + 'â€¦'\n","\n","p = Pipeline('vietnamese', embedding='xlm-roberta-large')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dtdjPqwZnbI0"},"source":["def algin_segmented_answer(tokenized_context, answer):\n","    answer_start = answer['answer_start']\n","    answer_text = ' '.join(answer['text'].split())\n","    answer_end = answer_start + len(answer_text)\n","\n","    left_spans, right_spans = [], []\n","    for sentence in tokenized_context['sentences']:\n","        for token in sentence['tokens']:\n","            left_spans.append(token['dspan'][0])\n","            right_spans.append(token['dspan'][1])\n","    \n","    if answer_start is not None and answer_start not in left_spans:\n","        corrected_answer_start = left_spans[0]\n","        corrected_answer_start_idx = 0\n","        for idx, left_span in enumerate(left_spans):\n","            if left_span > answer_start: break\n","            corrected_answer_start = left_span\n","            corrected_answer_start_idx = idx\n","        \n","        if corrected_answer_start_idx + 1 < len(left_spans) and ((corrected_answer_start + left_spans[corrected_answer_start_idx + 1])/2 < answer_start):\n","            answer_start = left_spans[corrected_answer_start_idx + 1]\n","        else:\n","            answer_start = corrected_answer_start\n","\n","    if answer_start is not None and answer_end not in right_spans:\n","        corrected_answer_end = right_spans[-1]\n","        corrected_answer_end_idx = 0\n","        for idx, right_span in enumerate(right_spans[::-1]):\n","            if right_span < answer_end: break\n","            corrected_answer_end = right_span\n","            corrected_answer_end_idx = idx\n","\n","        if corrected_answer_end_idx + 1 < len(right_spans) and ((corrected_answer_end + right_spans[::-1][corrected_answer_end_idx + 1])/2 < answer_end):\n","            answer_end = right_spans[::-1][corrected_answer_end_idx + 1]\n","        else:\n","            answer_end = corrected_answer_end\n","\n","    if answer_start is not None:\n","        while answer_start < len(tokenized_context['text']):\n","            if (tokenized_context['text'][answer_start] in punctuation or tokenized_context['text'][answer_start] == ' '): answer_start += 1\n","            else: break\n","        while answer_start not in left_spans: answer_start -= 1\n","\n","        while answer_end - 1 > 0:\n","            if tokenized_context['text'][answer_end - 1] in punctuation or tokenized_context['text'][answer_end - 1] == ' ': answer_end -= 1\n","            else: break\n","        while answer_end not in right_spans: answer_end += 1\n","\n","    cur_offset, new_answer_start, new_answer_end, new_answer_text = 0, None, None, []\n","    found_answer = False\n","    idx_sentence_begin = None\n","    idx_sentence_end = None\n","    for idx, sentence in enumerate(tokenized_context['sentences']):\n","        if not found_answer: cur_offset = 0\n","        for token in sentence['tokens']:\n","            if token['dspan'][0] == answer_start:\n","                new_answer_start = cur_offset\n","                found_answer = True\n","                idx_sentence_begin = idx\n","\n","            if new_answer_start is not None and new_answer_end is None:\n","                new_answer_text.append(token['text'].replace(' ', '_'))\n","\n","            if token['dspan'][1] == answer_end:\n","                new_answer_end = cur_offset\n","                idx_sentence_end = idx\n","\n","            cur_offset = cur_offset + token['dspan'][1] - token['dspan'][0] + 1\n","\n","    new_context_text = []\n","    for idx, sentence in enumerate(tokenized_context['sentences']):\n","        if idx_sentence_begin <= idx and idx <= idx_sentence_end:\n","            for token in sentence['tokens']:\n","                new_context_text.append(token['text'].replace(' ', '_'))\n","    \n","    new_context_text = ' '.join(new_context_text)\n","    new_answer_text = ' '.join(new_answer_text)\n","\n","    return new_context_text, {'answer_start': new_answer_start, 'text': new_answer_text}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jHPjl-J_okMl"},"source":["original_data = json.load(open('./data/train_data/original_data/train.json', 'r'))\n","\n","new_data = []\n","\n","for article in tqdm(original_data['data']):\n","    for paragraph in article['paragraphs']:\n","        context = paragraph['context']\n","        context = ' '.join(context.split())\n","        tokenized_context = p.tokenize(context)\n","        for qa in paragraph['qas']:\n","            question = qa['question']\n","            question = ' '.join(question.split())\n","            tokenized_question = p.tokenize(question, is_sent=True)\n","\n","            if 'plausible_answers' in qa:\n","                for answer in qa['plausible_answers']:\n","                    new_context_text, new_answer = algin_segmented_answer(tokenized_context, answer)\n","                    if new_context_text == '':\n","                        print(qa['id'])\n","                        continue\n","                    new_data.append({\n","                        'title': article['title'],\n","                        'paragraphs': [{\n","                            'qas': [\n","                                {\n","                                    'question': ' '.join([token['text'].replace(' ', '_') for token in tokenized_question['tokens']]),\n","                                    'answers': [],\n","                                    'plausible_answers': [new_answer],\n","                                    'id': qa['id'],\n","                                    'is_impossible': qa['is_impossible'],\n","                                }\n","                            ],\n","                            'context': new_context_text\n","                        }]\n","                    })\n","            else:\n","                for answer in qa['answers']:\n","                    new_context_text, new_answer = algin_segmented_answer(tokenized_context, answer)\n","                    if new_context_text == '':\n","                        print(qa['id'])\n","                        continue\n","                    new_data.append({\n","                        'title': article['title'],\n","                        'paragraphs': [{\n","                            'qas': [\n","                                {\n","                                    'question': ' '.join([token['text'].replace(' ', '_') for token in tokenized_question['tokens']]),\n","                                    'answers': [new_answer],\n","                                    'id': qa['id'],\n","                                    'is_impossible': qa['is_impossible'],\n","                                }\n","                            ],\n","                            'context': new_context_text              \n","                        }]\n","                    })\n","\n","json.dump(\n","    {\n","        'version': 'viquad2_top1_training_set',\n","        'data': new_data\n","    },\n","    open(f'./data/train_data/tokenized_data/segmented_top_1_data.json', 'w', encoding='utf-8'), ensure_ascii=False\n",")"],"execution_count":null,"outputs":[]}]}