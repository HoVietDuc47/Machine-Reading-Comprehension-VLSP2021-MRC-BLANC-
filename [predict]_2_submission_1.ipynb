{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[predict]_2_submission_1.ipynb","provenance":[],"collapsed_sections":["uoCwU4pVosoH"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"o_uDxHyhoTAf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635412663305,"user_tz":-420,"elapsed":20096,"user":{"displayName":"Vũ Nguyễn Đức","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08568379512742239702"}},"outputId":"f9b9dd37-f697-4f82-c6b1-e1eb1ab25ed4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"mlfKYyrBodKN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635412674903,"user_tz":-420,"elapsed":9145,"user":{"displayName":"Vũ Nguyễn Đức","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08568379512742239702"}},"outputId":"4d6d9a2a-5221-48ff-f469-e0922bd3741c"},"source":["!pip install datasets sacremoses huggingface_hub tokenizers sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n","\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20 kB 18.9 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 51 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 61 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 71 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 92 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 102 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 112 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 122 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 133 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 143 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 163 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 174 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 184 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 194 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 204 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 215 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 225 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 235 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 245 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 256 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 266 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 276 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 286 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 290 kB 10.5 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 48.0 MB/s \n","\u001b[?25hCollecting huggingface_hub\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s \n","\u001b[?25hCollecting tokenizers\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 47.1 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 54.9 MB/s \n","\u001b[?25hCollecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 64.8 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 44.6 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 48.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.7.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.3.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.13)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 57.8 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Collecting async-timeout<4.0,>=3.0\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 59.2 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, huggingface-hub, tokenizers, sentencepiece, sacremoses, datasets\n","Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.14.0 fsspec-2021.10.1 huggingface-hub-0.0.19 multidict-5.2.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.10.3 xxhash-2.0.2 yarl-1.7.0\n"]}]},{"cell_type":"code","metadata":{"id":"Uay9-3wNofQm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635412674904,"user_tz":-420,"elapsed":11,"user":{"displayName":"Vũ Nguyễn Đức","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08568379512742239702"}},"outputId":"5d6f52c4-1611-4a73-d823-6e1461a9794c"},"source":["cd /content/drive/MyDrive/VLSP2021-MRC-BLANC"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/VLSP2021-MRC-BLANC\n"]}]},{"cell_type":"markdown","metadata":{"id":"uoCwU4pVosoH"},"source":["## Predict"]},{"cell_type":"code","metadata":{"id":"zMZSQtj4oupE"},"source":["!python run_squad.py \\\n","  --model_type xlm-roberta \\\n","  --model_name_or_path ./models/finetuned5_models/xlm-roberta-large-fold_1 \\\n","  --do_eval \\\n","  --predict_file ./data/private_test_data/private_test_syllable.json \\\n","  --per_gpu_train_batch_size 4 \\\n","  --per_gpu_eval_batch_size 4 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3.0 \\\n","  --max_seq_length 384 \\\n","  --max_answer_length 500 \\\n","  --max_query_length 128 \\\n","  --doc_stride 128 \\\n","  --geometric_p 0.7 \\\n","  --window_size 2 \\\n","  --lmb 0.4 \\\n","  --logging_steps 500 \\\n","  --save_steps 500 \\\n","  --version_2_with_negative \\\n","  --cache_dir  ./results/private_test_result/5_fold/xlm-roberta-large-fold_1 \\\n","  --output_dir ./results/private_test_result/5_fold/xlm-roberta-large-fold_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y1A5SSpfo5Rw"},"source":["!python run_squad.py \\\n","  --model_type rembert \\\n","  --model_name_or_path ./models/finetuned5_models/rembert-fold_1 \\\n","  --do_eval \\\n","  --predict_file ./data/private_test_data/private_test_syllable.json \\\n","  --per_gpu_eval_batch_size 2 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3.0 \\\n","  --max_answer_length 500 \\\n","  --max_seq_length 384 \\\n","  --max_query_length 128 \\\n","  --doc_stride 128 \\\n","  --geometric_p 0.7 \\\n","  --window_size 2 \\\n","  --lmb 0.4 \\\n","  --logging_steps 500 \\\n","  --save_steps 0 \\\n","  --version_2_with_negative \\\n","  --overwrite_cache \\\n","  --cache_dir ./results/private_test_result/5_fold/rembert-fold_1 \\\n","  --output_dir ./results/private_test_result/5_fold/rembert-fold_1\\"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jTCg5VuBqb0N"},"source":["## Ensemble"]},{"cell_type":"code","metadata":{"id":"1FzYSl4r5Y_X"},"source":["n_fold = 5\n","input_nbest_files_rembert = [f\"./results/private_test_result/5_fold/rembert-fold_{i+1}/nbest_predictions_.json\" for i in range(n_fold)]\n","input_nbest_files_xlm = [f\"./results/private_test_result/5_fold/xlm-roberta-large-fold_{i+1}/nbest_predictions_.json\" for i in range(n_fold)]\n","input_nbest_files=input_nbest_files_rembert + input_nbest_files_xlm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pxw_n2P747iO"},"source":["import collections\n","import json\n","\n","idx = 0\n","best_cof = [0.4/n_fold for _ in range(n_fold)] + [0.64/n_fold for _ in range(n_fold)]\n"," \n","all_nbest = collections.OrderedDict()\n","for input_file in input_nbest_files:\n","    with open(input_file, \"r\") as reader:\n","        input_data = json.load(reader, strict=False)\n","        for (key, entries) in input_data.items():\n","            if key not in all_nbest:\n","                all_nbest[key] = collections.defaultdict(float)\n","            for entry in entries:\n","                all_nbest[key][entry[\"text\"]] += best_cof[idx] * entry[\"probability\"]\n","    idx += 1\n","\n","output_predictions = {}\n","for (key, entry_map) in all_nbest.items():\n","    sorted_texts = sorted(\n","        entry_map.keys(), key=lambda x: entry_map[x], reverse=True)\n","    best_text = sorted_texts[0]\n","    output_predictions[key] = best_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"__m1qxJGnvZ_"},"source":["file_out = \"/content/results_pred.json\"\n","with open(file_out, 'w', encoding='utf-8') as outfile:\n","        json.dump(output_predictions, outfile ,ensure_ascii=False,indent=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2TnGFIUhrBET"},"source":["## Keep the highest probability among duplicate answer "]},{"cell_type":"code","metadata":{"id":"jUHIg7-srDIq"},"source":["def group_ans_in_context(test_file):\n","  public_test = json.load(open(test_file))['data']\n","  i = 0\n","  data_id = {}\n","  for dt in public_test:\n","    for para in dt['paragraphs']:\n","      a = []\n","      for qa in para['qas']:\n","        a.append(qa['id'])\n","      data_id[i] = a\n","      i += 1\n","  return data_id\n"," \n","def find_best_prediction(all_nbest):\n","  best_predictions = {}\n","  for (key, entry_map) in all_nbest.items():\n","      sorted_texts = sorted(\n","          entry_map.keys(), key=lambda x: entry_map[x], reverse=True)\n","      best_text = sorted_texts[0]\n","      best_predictions[key] = entry_map[best_text]\n","  return best_predictions\n","\n","def find_same_predict_answer(data_id, data, best_predictions ):\n","    occurrences = lambda s, lst: (i for i,e in enumerate(lst) if e == s)\n","    dct_final = {}\n","    for key, values in data_id.items():\n","        predict = []\n","        for value in values:\n","          predict.append(data[value])\n","        lst_same_ans = []\n","        flag = False\n","        for value in values:\n","          if data[value] != \"\":\n","            lst = list(occurrences(data[value], predict))\n","            if len(lst) > 1:\n","              d = {}\n","              for i in lst:\n","                flag = True\n","                d[values[i]] = best_predictions[values[i]]\n","              if d not in lst_same_ans:\n","                lst_same_ans.append(d)\n","        if flag == True:\n","          dct_final[key] = lst_same_ans\n","        flag = False\n","    return dct_final\n","\n","def find_null_ans(dct_final): \n","    \"\"\"Find the worst predict answer among the same answer in each context\"\"\"\n","    null_ans = []\n","    for key, values in dct_final.items():\n","      for value in values:\n","        sorted_texts = sorted(\n","            value.keys(), key=lambda x: value[x], reverse=True)\n","        worst_text = sorted_texts[-1]\n","        null_ans.append(worst_text)\n","        if len(sorted_texts)>2:\n","          worst_text = sorted_texts[-2]\n","          null_ans.append(worst_text)\n","    return null_ans\n","\n","def delete_answers(predict_file_path, test_file_path, all_nbest):\n","    data = json.load(open(predict_file_path)) # Load data in predicted file\n","    data_id = group_ans_in_context(test_file_path) # Group answers id in the same context together\n","    best_prediction = find_best_prediction(all_nbest) # Find the highest probability predict answer in each context\n","    dct_final = find_same_predict_answer(data_id, data, best_prediction) # Find the probability in the same answer in each context\n","    null_ans = find_null_ans(dct_final) # Find the worst predict answer among the same answer in each context\n","    for k, v in data.items(): \n","      if k in null_ans:\n","        data[k] = \"\"\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGeX5rQmrLng"},"source":["results = delete_answers(\"/content/results_pred.json\", \"./data/private_test_data/private_test.json\", all_nbest)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"__K5RBI0rY24"},"source":["file_out = \"/content/results.json\"\n","with open(file_out, 'w', encoding='utf-8') as outfile:\n","        json.dump(results, outfile ,ensure_ascii=False,indent=4)"],"execution_count":null,"outputs":[]}]}