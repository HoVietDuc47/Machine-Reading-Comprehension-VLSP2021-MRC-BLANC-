{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[train]_4_create_k_fold_top1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ETvSByTAoYWI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638972161911,"user_tz":-420,"elapsed":22300,"user":{"displayName":"Hang Le Thi Thu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01131064715747594997"}},"outputId":"d0a8d877-51b3-45e0-f394-f584af6a429e"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"YY-6kLM6oZBF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638972163109,"user_tz":-420,"elapsed":8,"user":{"displayName":"Hang Le Thi Thu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01131064715747594997"}},"outputId":"430f27a1-6f51-4fca-b1c7-a1a0bb934271"},"source":["%cd '/content/gdrive/MyDrive/VLSP2021-MRC-BLANC'"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/1WQcuxpxeN4XEvs7OGyXFqtypm6n8iNvd/VLSP2021-MRC-BLANC\n"]}]},{"cell_type":"code","metadata":{"id":"gPP1dhkks7pE","executionInfo":{"status":"ok","timestamp":1638972307319,"user_tz":-420,"elapsed":7464,"user":{"displayName":"Hang Le Thi Thu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01131064715747594997"}}},"source":["import json\n","from sklearn.model_selection import KFold\n","\n","for k in [5, 10]:\n","    segmented_data = json.load(open('./data/train_data/tokenized_data/segmented_top_1_data.json', 'r'))['data']\n","\n","    kf = KFold(n_splits=k)\n","    kf.get_n_splits(segmented_data)\n","\n","    for idx, (train_index, dev_index) in enumerate(kf.split(segmented_data)):\n","        fold_idx = idx + 1\n","        json.dump({'version': f'train_{fold_idx}', 'data': [segmented_data[i] for i in train_index]},\n","                  open(f'./data/train_data/top1_data/{k}_fold_word/train_{fold_idx}.json', 'w', encoding='utf-8'), ensure_ascii=False)\n","\n","        json.dump({'version': f'dev_{fold_idx}', 'data': [segmented_data[i] for i in dev_index]},\n","                  open(f'./data/train_data/top1_data/{k}_fold_word/dev_{fold_idx}.json', 'w', encoding='utf-8'), ensure_ascii=False)\n","        \n","    for article in segmented_data:\n","        title = article['title']\n","        for paragraph in article['paragraphs']:\n","            paragraph['context'] = paragraph['context'].replace('_', ' ')\n","            for qa in paragraph['qas']:\n","                qa['question'] = qa['question'].replace('_', ' ')\n","                qa['answers'] = [{'answer_start': answer['answer_start'], 'text': answer['text'].replace('_', ' ')} for answer in qa['answers']]\n","                if 'plausible_answers' in qa:\n","                    qa['plausible_answers'] = [{'answer_start': answer['answer_start'], 'text': answer['text'].replace('_', ' ')} for answer in qa['plausible_answers']]\n","\n","    for idx, (train_index, dev_index) in enumerate(kf.split(segmented_data)):\n","        fold_idx = idx + 1\n","        json.dump({'version': f'train_{fold_idx}', 'data': [segmented_data[i] for i in train_index]},\n","                  open(f'./data/train_data/top1_data/{k}_fold_syllable/train_{fold_idx}.json', 'w', encoding='utf-8'), ensure_ascii=False)\n","\n","        json.dump({'version': f'dev_{fold_idx}', 'data': [segmented_data[i] for i in dev_index]},\n","                  open(f'./data/train_data/top1_data/{k}_fold_syllable/dev_{fold_idx}.json', 'w', encoding='utf-8'), ensure_ascii=False)"],"execution_count":5,"outputs":[]}]}