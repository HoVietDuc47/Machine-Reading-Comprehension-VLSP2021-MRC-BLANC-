{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[train]_5_script_for_training.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Y7S6I0iDeTbY"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_ACmsKl5QqT"},"source":["%cd '/content/gdrive/MyDrive/VLSP2021-MRC-BLANC'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqjMqSqUJ3Qb"},"source":["# We did not install transformers by pip command. We used transformers directly from the modified version by adding BLANC (https://github.com/yeonsw/BLANC).\n","!pip install datasets sacremoses huggingface_hub tokenizers sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8LtO3fjeVeY"},"source":["!python run_squad.py \\\n","  --model_type xlm-roberta \\\n","  --model_name_or_path ./models/pretrained_models/xlm-roberta-large/ \\\n","  --do_train \\\n","  --do_eval \\\n","  --eval_all_checkpoints \\\n","  --train_file ./data/train_data/full_data/5_fold_syllable/train_1.json \\\n","  --predict_file ./data/train_data/full_data/5_fold_syllable/dev_1.json \\\n","  --per_gpu_train_batch_size 4 \\\n","  --per_gpu_eval_batch_size 4 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3.0 \\\n","  --max_seq_length 384 \\\n","  --max_query_length 128 \\\n","  --doc_stride 128 \\\n","  --geometric_p 0.7 \\\n","  --window_size 2 \\\n","  --lmb 0.4 \\\n","  --logging_steps 500 \\\n","  --save_steps 0 \\\n","  --version_2_with_negative \\\n","  --overwrite_cache \\\n","  --cache_dir ./models/finetuned5_models/xlm-roberta-large-fold_1 \\\n","  --output_dir ./models/finetuned5_models/xlm-roberta-large-fold_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FdEHwX8Zxff4"},"source":["!python run_squad.py \\\n","  --model_type xlm-roberta \\\n","  --model_name_or_path ./models/pretrained_models/xlm-roberta-large/ \\\n","  --do_train \\\n","  --do_eval \\\n","  --eval_all_checkpoints \\\n","  --train_file ./data/train_data/full_data/10_fold_syllable/train_1.json \\\n","  --predict_file ./data/train_data/full_data/10_fold_syllable/dev_1.json \\\n","  --per_gpu_train_batch_size 4 \\\n","  --per_gpu_eval_batch_size 4 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3.0 \\\n","  --max_seq_length 384 \\\n","  --max_query_length 128 \\\n","  --doc_stride 128 \\\n","  --geometric_p 0.7 \\\n","  --window_size 2 \\\n","  --lmb 0.4 \\\n","  --logging_steps 500 \\\n","  --save_steps 0 \\\n","  --version_2_with_negative \\\n","  --overwrite_cache \\\n","  --cache_dir ./models/finetuned10_models/xlm-roberta-large-fold_1 \\\n","  --output_dir ./models/finetuned10_models/xlm-roberta-large-fold_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cJ0By0X4xwQY"},"source":["!python run_squad.py \\\n","  --model_type xlm-roberta \\\n","  --model_name_or_path ./models/pretrained_models/xlm-roberta-large/ \\\n","  --do_train \\\n","  --do_eval \\\n","  --eval_all_checkpoints \\\n","  --train_file ./data/train_data/top1_data/10_fold_syllable/train_1.json \\\n","  --predict_file ./data/train_data/top1_data/10_fold_syllable/dev_1.json \\\n","  --per_gpu_train_batch_size 4 \\\n","  --per_gpu_eval_batch_size 4 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3.0 \\\n","  --max_seq_length 384 \\\n","  --max_query_length 128 \\\n","  --doc_stride 128 \\\n","  --geometric_p 0.7 \\\n","  --window_size 2 \\\n","  --lmb 0.4 \\\n","  --logging_steps 500 \\\n","  --save_steps 0 \\\n","  --version_2_with_negative \\\n","  --overwrite_cache \\\n","  --cache_dir ./models/finetuned10_top1_models/xlm-roberta-large-fold_1 \\\n","  --output_dir ./models/finetuned10_top1_models/xlm-roberta-large-fold_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lTFSfzKlV8Fl"},"source":["!python run_squad.py \\\n","  --model_type rembert \\\n","  --model_name_or_path ./models/pretrained_models/rembert/ \\\n","  --do_train \\\n","  --do_eval \\\n","  --eval_all_checkpoints \\\n","  --train_file ./data/train_data/full_data/5_fold_syllable/train_1.json \\\n","  --predict_file ./data/train_data/full_data/5_fold_syllable/dev_1.json \\\n","  --per_gpu_train_batch_size 2 \\\n","  --per_gpu_eval_batch_size 2 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3.0 \\\n","  --max_seq_length 384 \\\n","  --max_query_length 128 \\\n","  --doc_stride 128 \\\n","  --geometric_p 0.7 \\\n","  --window_size 2 \\\n","  --lmb 0.4 \\\n","  --logging_steps 500 \\\n","  --save_steps 0 \\\n","  --version_2_with_negative \\\n","  --overwrite_cache \\\n","  --cache_dir ./models/finetuned5_models/rembert-fold_1 \\\n","  --output_dir ./models/finetuned5_models/rembert-fold_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TZ7_0q7HxkMH"},"source":["!python run_squad.py \\\n","  --model_type rembert \\\n","  --model_name_or_path ./models/pretrained_models/rembert/ \\\n","  --do_train \\\n","  --do_eval \\\n","  --eval_all_checkpoints \\\n","  --train_file ./data/train_data/full_data/10_fold_syllable/train_1.json \\\n","  --predict_file ./data/train_data/full_data/10_fold_syllable/dev_1.json \\\n","  --per_gpu_train_batch_size 2 \\\n","  --per_gpu_eval_batch_size 2 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3.0 \\\n","  --max_seq_length 384 \\\n","  --max_query_length 128 \\\n","  --doc_stride 128 \\\n","  --geometric_p 0.7 \\\n","  --window_size 2 \\\n","  --lmb 0.4 \\\n","  --logging_steps 500 \\\n","  --save_steps 0 \\\n","  --version_2_with_negative \\\n","  --overwrite_cache \\\n","  --cache_dir ./models/finetuned10_models/rembert-fold_1 \\\n","  --output_dir ./models/finetuned10_models/rembert-fold_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQUjA2mxx4Fh"},"source":["!python run_squad.py \\\n","  --model_type rembert \\\n","  --model_name_or_path ./models/pretrained_models/rembert/ \\\n","  --do_train \\\n","  --do_eval \\\n","  --eval_all_checkpoints \\\n","  --train_file ./data/train_data/top1_data/10_fold_syllable/train_1.json \\\n","  --predict_file ./data/train_data/top1_data/10_fold_syllable/dev_1.json \\\n","  --per_gpu_train_batch_size 2 \\\n","  --per_gpu_eval_batch_size 2 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3.0 \\\n","  --max_seq_length 384 \\\n","  --max_query_length 128 \\\n","  --doc_stride 128 \\\n","  --geometric_p 0.7 \\\n","  --window_size 2 \\\n","  --lmb 0.4 \\\n","  --logging_steps 500 \\\n","  --save_steps 0 \\\n","  --version_2_with_negative \\\n","  --overwrite_cache \\\n","  --cache_dir ./models/finetuned10_top1_models/rembert-fold_1 \\\n","  --output_dir ./models/finetuned10_top1_models/rembert-fold_1"],"execution_count":null,"outputs":[]}]}